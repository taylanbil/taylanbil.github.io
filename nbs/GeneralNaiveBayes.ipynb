{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn-like api for a general implementation of Naive Bayes\n",
    "\n",
    "## 0. Intro:\n",
    "\n",
    "Although the computations that go into it are very tedious Naive Bayes is one of the more accessible ML classification algorithms out there. The reason for that is it is easy to understand, and it makes __sense__, intuitively speaking.\n",
    "\n",
    "However, the `sklearn` implementations of Naive Bayes are built (excluding `GaussianNB`) with certain assumptions, which make them tough to use without a lot of pre-processing, as we explored in [this post](/multinbvsbinomnb/). This time, I'd like to implement the Naive Bayes algorithm idea for a general input dataset. I will not optimize the code, so it won't be naturally scalable or anything, the goal is just to hack something together quickly.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Refresher on Naive Bayes\n",
    "\n",
    "Let's start with revisiting the algorithm. In the following dataset, there are 4 columns to predict whether someone will play tennis on that day. Let's take a quick look at this small dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outlook</th>\n",
       "      <th>temp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind</th>\n",
       "      <th>play</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Strong</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Strong</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Strong</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Mild</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Mild</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Strong</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Strong</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Hot</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Strong</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     outlook  temp humidity    wind play\n",
       "0      Sunny   Hot     High    Weak   No\n",
       "1      Sunny   Hot     High  Strong   No\n",
       "2   Overcast   Hot     High    Weak  Yes\n",
       "3       Rain  Mild     High    Weak  Yes\n",
       "4       Rain  Cool   Normal    Weak  Yes\n",
       "5       Rain  Cool   Normal  Strong   No\n",
       "6   Overcast  Cool   Normal  Strong  Yes\n",
       "7      Sunny  Mild     High    Weak   No\n",
       "8      Sunny  Cool   Normal    Weak  Yes\n",
       "9       Rain  Mild   Normal    Weak  Yes\n",
       "10     Sunny  Mild   Normal  Strong  Yes\n",
       "11  Overcast  Mild     High  Strong  Yes\n",
       "12  Overcast   Hot   Normal    Weak  Yes\n",
       "13      Rain  Mild     High  Strong   No"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "\n",
    "# A really simple dataset to demonstrate the algorithm\n",
    "data = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/petehunt/c4.5-compiler/master/example/tennis.csv',\n",
    "    usecols=['outlook', 'temp', 'humidity', 'wind', 'play']\n",
    ")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this dataset, we can use Bayesian thinking to predict whether or not someone will play tennis on a new day, given the weather. Specifically, Naive Bayes proceeds as follows:\n",
    "\n",
    "\n",
    "Let's say, on day **14**, the weather is \n",
    "\n",
    "| outlook  | temp  | humidity  | wind  |\n",
    "|---|---|---|---|\n",
    "|  Overcast | Mild  | Normal  | Weak  |  \n",
    "\n",
    "\n",
    "We want to compute the following probabilities:\n",
    "\n",
    "$$ P(yes \\,|\\,  overcast, mild, normal, weak)$$\n",
    "\n",
    "$$ P(no \\,|\\, overcast, mild, normal, weak) $$\n",
    "\n",
    "Instead of computing these exactly, we compute proxies of these. Since this part is supposed to be a refresher only, I'll only include the computation of the first case. It goes like this:\n",
    "\n",
    "\n",
    "$$ P(yes \\,|\\, o, m, n, w) \\approx P(o | y)\\times P(m|y) \\times P(n|y) \\times P(w|y) \\times P(y) $$\n",
    "\n",
    "And the terms on the right side of the equation can be derived from looking at the dataset and counting. For example, \n",
    "\n",
    "$$ P(mild \\,|\\, yes)$$\n",
    "\n",
    "is just the count of total days with __play__ = _yes_ and __temperature__ = _mild_, divided by the days with __play__ = _yes_. Let's compute;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(mild | yes) = 4/9\n"
     ]
    }
   ],
   "source": [
    "mild = data.temp == 'Mild'\n",
    "yes = data.play == 'Yes'\n",
    "\n",
    "print('P(mild | yes) = {}/{}'.format((mild&yes).sum(), yes.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final term in the formula above is $P(y)$. That's even simpler to compute, since it is just the count of lines with _yes_ divided by the total number of lines.\n",
    "\n",
    "Let's put the pieces together by computing proxies for both probabilities with code. We get;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(yes | o, m, n, w) approximately is 0.79012345679\n",
      "P(no | o, m, n, w) approximately is 0.0\n"
     ]
    }
   ],
   "source": [
    "yes = data.play == 'Yes'\n",
    "\n",
    "o = data.outlook == 'Overcast'\n",
    "m = data.temp == 'Mild'\n",
    "n = data.humidity == 'Normal'\n",
    "w = data.wind == 'Weak'\n",
    "\n",
    "ans = {\n",
    "    'yes': yes.sum(),\n",
    "    'no': (~yes).sum()\n",
    "}\n",
    "for elt in [o, m, n, w]:\n",
    "    ans['yes'] *= (yes & elt).sum() / yes.sum()\n",
    "    ans['no'] *= ((~yes) & elt).sum() / (~yes).sum()\n",
    "\n",
    "print('P(yes | o, m, n, w) approximately is {}'.format(ans['yes']))\n",
    "print('P(no | o, m, n, w) approximately is {}'.format(ans['no']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we end this section, notice that the algorithm thinks there's 0 chance that the person does not play tennis under these circumstances. Not only that's a strong opinion, but also it results in numerical difficulties in a real world implementation. To remedy this, one uses Laplace smoothing.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Implementation\n",
    "\n",
    "### 2.a Preprocessing\n",
    "\n",
    "You may have noticed that in our toy dataset, every attribute was categorical. There were no continuous numerical, nor ordinal fields. Also, unlike `sklearn`'s NB algorithms, we did not make any assumptions about the incoming training dataset. Columns were (thought to be) pairwise independent.\n",
    "\n",
    "As a design choice, we will present a version of Naive Bayes that works with such a training set. Every column will be categorical, and we will implement the computations we carried out above. In practice though, not every training data will consist ofcategorical columns only. To that end, let's first code a `Discretizer` object. The `Discretizer` will do the following:\n",
    "\n",
    "1. Takes in a continuous `pandas` series and bins the values.\n",
    "2. Remembers the cutoffs so that it can apply the same transformations later to find which bin the new value belongs to.\n",
    "\n",
    "Feel free to skip over the code for the usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from bisect import bisect_right\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Discretizer(object):\n",
    "\n",
    "    def __init__(self, upperlim=20, bottomlim=0, mapping=False):\n",
    "        self.mapping = mapping\n",
    "        self.set_lims(upperlim, bottomlim)\n",
    "\n",
    "    @property\n",
    "    def cutoffs(self):\n",
    "        return [i[0] for i in self.mapping]\n",
    "\n",
    "    def set_lims(self, upperlim, bottomlim):\n",
    "        if not self.mapping:\n",
    "            self.bottomlim = bottomlim\n",
    "            self.upperlim = upperlim\n",
    "        else:\n",
    "            vals = sorted(np.unique(map(itemgetter(1), self.mapping)))\n",
    "            self.bottomlim = vals[0]\n",
    "            self.upperlim = vals[-1]\n",
    "        assert self.bottomlim < self.upperlim\n",
    "\n",
    "    def fit(self, continuous_series, subsample=None):\n",
    "        self.mapping = []\n",
    "        continuous_series = pd.Series(continuous_series).reset_index(drop=True).dropna()\n",
    "        if subsample is not None:\n",
    "            n = len(continuous_series)*subsample if subsample < 1 else subsample\n",
    "            continuous_series = np.random.choice(continuous_series, n, replace=False)\n",
    "        ranked = pd.Series(continuous_series).rank(pct=1, method='average')\n",
    "        ranked *= self.upperlim - self.bottomlim\n",
    "        ranked += self.bottomlim\n",
    "        ranked = ranked.map(round)\n",
    "        nvals = sorted(np.unique(ranked))  # sorted in case numpy changes\n",
    "        for nval in nvals:\n",
    "            cond = ranked == nval\n",
    "            self.mapping.append((continuous_series[cond].min(), int(nval)))\n",
    "\n",
    "    def transform_single(self, val):\n",
    "        if not self.mapping:\n",
    "            raise NotImplementedError('Haven\\'t been fitted yet')\n",
    "        elif pd.isnull(val):\n",
    "            return None\n",
    "        i = bisect_right(self.cutoffs, val) - 1\n",
    "        if i == -1:\n",
    "            return 0\n",
    "        return self.mapping[i][1]\n",
    "\n",
    "    def transform(self, vals):\n",
    "        if isinstance(vals, float):\n",
    "            return self.transform_single(vals)\n",
    "        elif vals is None:\n",
    "            return None\n",
    "        return pd.Series(vals).map(self.transform_single)\n",
    "\n",
    "    def fit_transform(self, vals):\n",
    "        self.fit(vals)\n",
    "        return self.transform(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The api is similar to `sklearn`, with `fit`, `transform` and `fit_transform` methods. Let's see the usage on a simple example. You can check if the values of y are binned correctly by looking at the cutoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutoffs:\n",
      "[0.10316905194019832, 0.28798638771620189, 0.48254030703944994, 0.69792871673000578, 0.89006354032939106]\n",
      "------------------------------\n",
      "values:\n",
      "[ 0.50908875  0.00113411  0.73404021  0.3087637 ]\n",
      "------------------------------\n",
      "bins:\n",
      "0    3\n",
      "1    0\n",
      "2    4\n",
      "3    2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "D = Discretizer(upperlim=5)\n",
    "\n",
    "y = np.random.uniform(0, 1, 1000)\n",
    "D.fit(y)\n",
    "print('cutoffs:')\n",
    "print(D.cutoffs[1:])\n",
    "print('-'*30)\n",
    "print('values:')\n",
    "print(y[:4])\n",
    "print('-'*30)\n",
    "print('bins:')\n",
    "print(D.transform(y[:4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the part where we apply the `Discretizer` object to the whole dataset. To that end, we will define a `NaiveBayesPreprocessor` object. If a field is discrete (i.e. categorical), it will leave it (mostly) untouched (in reality, it will eliminate the values that does not occur more than 1% of the time). If the field is continuous, it will bin it as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NaiveBayesPreprocessor(object):\n",
    "    \"\"\"\n",
    "    Don't pass in Nans. fill with keyword.\n",
    "    \"\"\"\n",
    "\n",
    "    OTHER = '____OTHER____'\n",
    "    FILLNA = '____NA____'\n",
    "\n",
    "    def __init__(self, alpha=1.0, min_freq=0.01, bins=20):\n",
    "        self.alpha = alpha  # Laplace smoothing\n",
    "        self.min_freq = min_freq  # drop values occuring less frequently than this\n",
    "        self.bins = bins  # number of bins for continuous fields\n",
    "\n",
    "    def learn_continuous_transf(self, series):\n",
    "        D = Discretizer(upperlim=self.bins)\n",
    "        D.fit(series)\n",
    "        return D\n",
    "\n",
    "    def learn_discrete_transf(self, series):\n",
    "        vcs = series.value_counts(dropna=False, normalize=True)\n",
    "        vcs = vcs[vcs >= self.min_freq]\n",
    "        keep = set(vcs.index)\n",
    "        transf = lambda r: r if r in keep else self.OTHER\n",
    "        return transf\n",
    "\n",
    "    def learn_transf(self, series):\n",
    "        if series.dtype == np.float64:\n",
    "            return self.learn_continuous_transf(series)\n",
    "        else:\n",
    "            return self.learn_discrete_transf(series)\n",
    "\n",
    "    def fit(self, X_orig, y=None):\n",
    "        \"\"\"\n",
    "        Expects pandas series and pandas DataFrame\n",
    "        \"\"\"\n",
    "        # get dtypes\n",
    "        self.dtypes = defaultdict(set)\n",
    "        for fld, dtype in X_orig.dtypes.iteritems():\n",
    "            self.dtypes[dtype].add(fld)\n",
    "\n",
    "        X = X_orig\n",
    "        # X = X_orig.fillna(self.FILLNA)\n",
    "        # get transfs\n",
    "        self.transformations = {\n",
    "            fld: self.learn_transf(series)\n",
    "            for fld, series in X.iteritems()}\n",
    "\n",
    "    def transform(self, X_orig, y=None):\n",
    "        \"\"\"\n",
    "        Expects pandas series and pandas DataFrame\n",
    "        \"\"\"\n",
    "        X = X_orig.copy()\n",
    "        # X = X_orig.fillna(self.FILLNA)\n",
    "        for fld, func in self.transformations.items():\n",
    "            if isinstance(func, Discretizer):\n",
    "                X[fld] = func.transform(X[fld])\n",
    "            else:\n",
    "                X[fld] = X[fld].map(func)\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see the preprocessor in action. Before that though, now is the time to code up the `NaiveBayesClassifier`.\n",
    "\n",
    "## 2.b. NaiveBayesClassifier\n",
    "\n",
    "The following class will implement the vanilla Naive Bayes algorithm as seen above. I will try to stick to a `sklearn`-like api. Once again, code is not optimal at all, the goal is to get to something working quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier(object):\n",
    "\n",
    "    def __init__(self, alpha=1.0, class_priors=None, **kwargs):\n",
    "        self.alpha = alpha\n",
    "        self.class_priors = class_priors\n",
    "\n",
    "    def get_class_log_priors(self, y):\n",
    "        self.classes_ = y.unique()\n",
    "        if self.class_priors is None:\n",
    "            self.class_priors = y.value_counts(normalize=1)\n",
    "        elif isinstance(self.class_priors, str) and self.class_priors == 'equal':\n",
    "            raise NotImplementedError\n",
    "        self.class_log_priors = self.class_priors.map(np.log)\n",
    "\n",
    "    def get_log_likelihoods(self, fld):\n",
    "        table = self.groups[fld].value_counts(dropna=False).unstack(fill_value=0)\n",
    "        table += self.alpha\n",
    "        sums = table.sum(axis=1)\n",
    "        likelihoods = table.apply(lambda r: r/sums, axis=0)\n",
    "        log_likelihoods = likelihoods.applymap(np.log)\n",
    "        return {k if pd.notnull(k) else None: v for k, v in log_likelihoods.items()}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y = pd.Series(y)\n",
    "        self.get_class_log_priors(y)\n",
    "        self.groups = X.groupby(y)\n",
    "        self.log_likelihoods = {\n",
    "            fld: self.get_log_likelihoods(fld)\n",
    "            for fld, series in X.iteritems()\n",
    "        }\n",
    "\n",
    "    def get_approx_log_posterior(self, series, class_):\n",
    "        log_posterior = self.class_log_priors[class_]  # prior\n",
    "        for fld, val in series.iteritems():\n",
    "            # there are cases where the `val` is not seen before\n",
    "            # as in having a `nan` in the scoring dataset,\n",
    "            #   but no `nans in the training set\n",
    "            # in those cases, we want to not add anything to the log_posterior\n",
    "\n",
    "            # This is to handle the Nones and np.nans etc.\n",
    "            val = val if pd.notnull(val) else None\n",
    "\n",
    "            if val not in self.log_likelihoods[fld]:\n",
    "                continue\n",
    "            log_posterior += self.log_likelihoods[fld][val][class_]\n",
    "        return log_posterior\n",
    "\n",
    "    def decision_function_series(self, series):\n",
    "        approx_log_posteriors = [\n",
    "            self.get_approx_log_posterior(series, class_)\n",
    "            for class_ in self.classes_]\n",
    "        return pd.Series(approx_log_posteriors, index=self.classes_)\n",
    "\n",
    "    def decision_function_df(self, df):\n",
    "        return df.apply(self.decision_function_series, axis=1)\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        returns the log posteriors\n",
    "        \"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            return self.decision_function_df(X)\n",
    "        elif isinstance(X, pd.Series):\n",
    "            return self.decision_function_series(X)\n",
    "        elif isinstance(X, dict):\n",
    "            return self.decision_function_series(pd.Series(X))\n",
    "\n",
    "    def predict_proba(self, X, normalize=True):\n",
    "        \"\"\"\n",
    "        returns the (normalized) posterior probability\n",
    "\n",
    "        normalization is just division by the evidence. doesn't change the argmax.\n",
    "        \"\"\"\n",
    "        log_post = self.decision_function(X)\n",
    "        if isinstance(log_post, pd.Series):\n",
    "            post = log_post.map(np.exp)\n",
    "        elif isinstance(log_post, pd.DataFrame):\n",
    "            post = log_post.applymap(np.exp)\n",
    "        else:\n",
    "            raise NotImplementedError('type of X is \"{}\"'.format(type(X)))\n",
    "        if normalize:\n",
    "            if isinstance(post, pd.Series):\n",
    "                post /= post.sum()\n",
    "            elif isinstance(post, pd.DataFrame):\n",
    "                post = post.div(post.sum(axis=1), axis=0)\n",
    "        return post\n",
    "\n",
    "    def predict(self, X):\n",
    "        probas = self.decision_function(X)\n",
    "        if isinstance(probas, pd.Series):\n",
    "            return np.argmax(probas)\n",
    "        return probas.apply(np.argmax, axis=1)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        preds = self.predict(X)\n",
    "        return np.mean(np.array(y) == preds.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example usage\n",
    "\n",
    "### 3.1. Wine quality dataset\n",
    "\n",
    "First example will work with the following [dataset](https://archive.ics.uci.edu/ml/datasets/wine+quality), hosted @ https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/.\n",
    "\n",
    "We will load it locally, and predict if a given wine is *red* or *white*. The reader would need to do minimal prework to get the dataset hosted in the link above in this format.\n",
    "\n",
    "Let's take a quick look at the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6497, 13)\n",
      "------------------------------\n",
      "white    0.753886\n",
      "red      0.246114\n",
      "Name: color, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2196</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.37</td>\n",
       "      <td>4.9</td>\n",
       "      <td>0.034</td>\n",
       "      <td>26.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>0.99280</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.8</td>\n",
       "      <td>6</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5327</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.33</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.028</td>\n",
       "      <td>21.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.99012</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.41</td>\n",
       "      <td>13.5</td>\n",
       "      <td>7</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2911</th>\n",
       "      <td>9.6</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.040</td>\n",
       "      <td>16.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0.99380</td>\n",
       "      <td>2.94</td>\n",
       "      <td>0.43</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  \\\n",
       "2196            7.0              0.29         0.37             4.9      0.034   \n",
       "5327            6.2              0.20         0.33             5.4      0.028   \n",
       "2911            9.6              0.25         0.54             1.3      0.040   \n",
       "\n",
       "      free_sulfur_dioxide  total_sulfur_dioxide  density    pH  sulphates  \\\n",
       "2196                 26.0                 127.0  0.99280  3.17       0.44   \n",
       "5327                 21.0                  75.0  0.99012  3.36       0.41   \n",
       "2911                 16.0                 160.0  0.99380  2.94       0.43   \n",
       "\n",
       "      alcohol  quality  color  \n",
       "2196     10.8        6  white  \n",
       "5327     13.5        7  white  \n",
       "2911     10.5        5  white  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winedata = pd.read_csv('~/metis/github/ct_intel_ml_curriculum/data/Wine_Quality_Data.csv')\n",
    "print(winedata.shape)\n",
    "print('-'*30)\n",
    "print(winedata.color.value_counts(normalize=True))\n",
    "winedata.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this dataset becomes once we transform it with the `NaiveBayesPreprocessor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3876</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2823</th>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  \\\n",
       "5998              7                 6            4              18         12   \n",
       "3876             13                 0            9               2          3   \n",
       "2823             12                 5           15               8          3   \n",
       "\n",
       "      free_sulfur_dioxide  total_sulfur_dioxide  density  pH  sulphates  \\\n",
       "5998                   13                    14       14   5         17   \n",
       "3876                    8                     6        6  19          3   \n",
       "2823                   10                     8        2  13         12   \n",
       "\n",
       "      alcohol  quality  color  \n",
       "5998        4        5  white  \n",
       "3876       12        6  white  \n",
       "2823       18        7  white  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NaiveBayesPreprocessor().fit_transform(winedata).sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the last column (which is categorical), is left untouched.\n",
    "\n",
    "--- \n",
    "\n",
    "Now let's try the Naive Bayes algorithms on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bernoulli</th>\n",
       "      <th>Gaussian</th>\n",
       "      <th>Multinomial</th>\n",
       "      <th>_vanilla_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.764308</td>\n",
       "      <td>0.973538</td>\n",
       "      <td>0.920615</td>\n",
       "      <td>0.987077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.754462</td>\n",
       "      <td>0.969231</td>\n",
       "      <td>0.913846</td>\n",
       "      <td>0.983385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.777231</td>\n",
       "      <td>0.971077</td>\n",
       "      <td>0.924308</td>\n",
       "      <td>0.988308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.777846</td>\n",
       "      <td>0.966154</td>\n",
       "      <td>0.915077</td>\n",
       "      <td>0.990769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.780308</td>\n",
       "      <td>0.965538</td>\n",
       "      <td>0.928615</td>\n",
       "      <td>0.989538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Bernoulli  Gaussian  Multinomial  _vanilla_\n",
       "0   0.764308  0.973538     0.920615   0.987077\n",
       "1   0.754462  0.969231     0.913846   0.983385\n",
       "2   0.777231  0.971077     0.924308   0.988308\n",
       "3   0.777846  0.966154     0.915077   0.990769\n",
       "4   0.780308  0.965538     0.928615   0.989538"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from collections import defaultdict\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = [('preprocessor', NaiveBayesPreprocessor(bins=20)),\n",
    "        ('nb', NaiveBayesClassifier())]\n",
    "pipe = Pipeline(pipe)\n",
    "nbs = {'_vanilla_': pipe,\n",
    "       'Multinomial': MultinomialNB(),\n",
    "       'Bernoulli': BernoulliNB(),\n",
    "       'Gaussian': GaussianNB()}\n",
    "\n",
    "X, y = winedata[winedata.columns[:-1]], winedata[winedata.columns[-1]]\n",
    "\n",
    "rs = ShuffleSplit(test_size=0.25, n_splits=5)\n",
    "scores = defaultdict(list)\n",
    "for train_index, test_index in rs.split(X):\n",
    "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "    for key, est in nbs.items():\n",
    "        est.fit(X_train, y_train)\n",
    "        scores[key].append(est.score(X_test, y_test))\n",
    "\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see above, Bernoulli and Multinomial Naive Bayes algorithms aren't performing well, simply because this dataset isn't suitable for their use, as explored in [the previous post](/multinbvsbinomnb/). `GaussianNB` performs OK, but is beaten by our implementation of NB. To satisfy the curious among us, let's throw `GradientBoostingClassifier` and `RandomForestClassifier` (without parameter tuning) at this;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GBT</th>\n",
       "      <th>RF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.994462</td>\n",
       "      <td>0.994462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.993846</td>\n",
       "      <td>0.995692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.995077</td>\n",
       "      <td>0.993846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.993231</td>\n",
       "      <td>0.993231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.993231</td>\n",
       "      <td>0.993846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        GBT        RF\n",
       "0  0.994462  0.994462\n",
       "1  0.993846  0.995692\n",
       "2  0.995077  0.993846\n",
       "3  0.993231  0.993231\n",
       "4  0.993231  0.993846"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "nbs = {'GBT': GradientBoostingClassifier(), 'RF': RandomForestClassifier()}\n",
    "scores = defaultdict(list)\n",
    "for train_index, test_index in rs.split(X):\n",
    "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "    for key, est in nbs.items():\n",
    "        est.fit(X_train, y_train)\n",
    "        scores[key].append(est.score(X_test, y_test))\n",
    "\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah these powerful algorithms yield better results. Let's move on to our second example.\n",
    "\n",
    "### 3.2. Human Activity Recognition using Smartphones\n",
    "\n",
    "Dataset and description can be found @ https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10299, 562)\n",
      "------------------------------\n",
      "LAYING                0.188756\n",
      "STANDING              0.185067\n",
      "SITTING               0.172541\n",
      "WALKING               0.167201\n",
      "WALKING_UPSTAIRS      0.149917\n",
      "WALKING_DOWNSTAIRS    0.136518\n",
      "Name: Activity, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tBodyAcc-mean()-X</th>\n",
       "      <th>tBodyAcc-mean()-Y</th>\n",
       "      <th>tBodyAcc-mean()-Z</th>\n",
       "      <th>tBodyAcc-std()-X</th>\n",
       "      <th>tBodyAcc-std()-Y</th>\n",
       "      <th>tBodyAcc-std()-Z</th>\n",
       "      <th>tBodyAcc-mad()-X</th>\n",
       "      <th>tBodyAcc-mad()-Y</th>\n",
       "      <th>tBodyAcc-mad()-Z</th>\n",
       "      <th>tBodyAcc-max()-X</th>\n",
       "      <th>...</th>\n",
       "      <th>fBodyBodyGyroJerkMag-skewness()</th>\n",
       "      <th>fBodyBodyGyroJerkMag-kurtosis()</th>\n",
       "      <th>angle(tBodyAccMean,gravity)</th>\n",
       "      <th>angle(tBodyAccJerkMean),gravityMean)</th>\n",
       "      <th>angle(tBodyGyroMean,gravityMean)</th>\n",
       "      <th>angle(tBodyGyroJerkMean,gravityMean)</th>\n",
       "      <th>angle(X,gravityMean)</th>\n",
       "      <th>angle(Y,gravityMean)</th>\n",
       "      <th>angle(Z,gravityMean)</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5882</th>\n",
       "      <td>0.275918</td>\n",
       "      <td>-0.018486</td>\n",
       "      <td>-0.100284</td>\n",
       "      <td>-0.993155</td>\n",
       "      <td>-0.958860</td>\n",
       "      <td>-0.962892</td>\n",
       "      <td>-0.993935</td>\n",
       "      <td>-0.953637</td>\n",
       "      <td>-0.960433</td>\n",
       "      <td>-0.934424</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.605982</td>\n",
       "      <td>-0.868852</td>\n",
       "      <td>0.048834</td>\n",
       "      <td>0.094434</td>\n",
       "      <td>-0.922991</td>\n",
       "      <td>0.715411</td>\n",
       "      <td>-0.833794</td>\n",
       "      <td>0.212639</td>\n",
       "      <td>0.007696</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>0.324749</td>\n",
       "      <td>-0.012294</td>\n",
       "      <td>-0.053416</td>\n",
       "      <td>-0.336888</td>\n",
       "      <td>0.157011</td>\n",
       "      <td>-0.387988</td>\n",
       "      <td>-0.409819</td>\n",
       "      <td>0.152768</td>\n",
       "      <td>-0.426130</td>\n",
       "      <td>-0.069544</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.252935</td>\n",
       "      <td>-0.653851</td>\n",
       "      <td>-0.294646</td>\n",
       "      <td>-0.449551</td>\n",
       "      <td>0.601912</td>\n",
       "      <td>-0.138953</td>\n",
       "      <td>-0.790079</td>\n",
       "      <td>0.242435</td>\n",
       "      <td>0.002180</td>\n",
       "      <td>WALKING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9466</th>\n",
       "      <td>0.287556</td>\n",
       "      <td>-0.018570</td>\n",
       "      <td>-0.108500</td>\n",
       "      <td>-0.984497</td>\n",
       "      <td>-0.981637</td>\n",
       "      <td>-0.987341</td>\n",
       "      <td>-0.985297</td>\n",
       "      <td>-0.982858</td>\n",
       "      <td>-0.987223</td>\n",
       "      <td>-0.927790</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.656437</td>\n",
       "      <td>-0.892907</td>\n",
       "      <td>0.034444</td>\n",
       "      <td>0.195809</td>\n",
       "      <td>0.352398</td>\n",
       "      <td>-0.039287</td>\n",
       "      <td>0.411160</td>\n",
       "      <td>-0.307855</td>\n",
       "      <td>-0.682102</td>\n",
       "      <td>LAYING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 562 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tBodyAcc-mean()-X  tBodyAcc-mean()-Y  tBodyAcc-mean()-Z  \\\n",
       "5882           0.275918          -0.018486          -0.100284   \n",
       "1263           0.324749          -0.012294          -0.053416   \n",
       "9466           0.287556          -0.018570          -0.108500   \n",
       "\n",
       "      tBodyAcc-std()-X  tBodyAcc-std()-Y  tBodyAcc-std()-Z  tBodyAcc-mad()-X  \\\n",
       "5882         -0.993155         -0.958860         -0.962892         -0.993935   \n",
       "1263         -0.336888          0.157011         -0.387988         -0.409819   \n",
       "9466         -0.984497         -0.981637         -0.987341         -0.985297   \n",
       "\n",
       "      tBodyAcc-mad()-Y  tBodyAcc-mad()-Z  tBodyAcc-max()-X    ...     \\\n",
       "5882         -0.953637         -0.960433         -0.934424    ...      \n",
       "1263          0.152768         -0.426130         -0.069544    ...      \n",
       "9466         -0.982858         -0.987223         -0.927790    ...      \n",
       "\n",
       "      fBodyBodyGyroJerkMag-skewness()  fBodyBodyGyroJerkMag-kurtosis()  \\\n",
       "5882                        -0.605982                        -0.868852   \n",
       "1263                        -0.252935                        -0.653851   \n",
       "9466                        -0.656437                        -0.892907   \n",
       "\n",
       "      angle(tBodyAccMean,gravity)  angle(tBodyAccJerkMean),gravityMean)  \\\n",
       "5882                     0.048834                              0.094434   \n",
       "1263                    -0.294646                             -0.449551   \n",
       "9466                     0.034444                              0.195809   \n",
       "\n",
       "      angle(tBodyGyroMean,gravityMean)  angle(tBodyGyroJerkMean,gravityMean)  \\\n",
       "5882                         -0.922991                              0.715411   \n",
       "1263                          0.601912                             -0.138953   \n",
       "9466                          0.352398                             -0.039287   \n",
       "\n",
       "      angle(X,gravityMean)  angle(Y,gravityMean)  angle(Z,gravityMean)  \\\n",
       "5882             -0.833794              0.212639              0.007696   \n",
       "1263             -0.790079              0.242435              0.002180   \n",
       "9466              0.411160             -0.307855             -0.682102   \n",
       "\n",
       "      Activity  \n",
       "5882  STANDING  \n",
       "1263   WALKING  \n",
       "9466    LAYING  \n",
       "\n",
       "[3 rows x 562 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activity_data = pd.read_csv('~/metis/github/ct_intel_ml_curriculum/data/Human_Activity_Recognition_Using_Smartphones_Data.csv')\n",
    "print(activity_data.shape)\n",
    "print('-'*30)\n",
    "print(activity_data.Activity.value_counts(normalize=True))\n",
    "activity_data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this dataset has negative numbers in it, `MultinomialNB` will not work with it. We can add the minimum value to it and make it work, but again, it just doesn't make a lot of sense to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bernoulli</th>\n",
       "      <th>Gaussian</th>\n",
       "      <th>_vanilla_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.850485</td>\n",
       "      <td>0.725049</td>\n",
       "      <td>0.793398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.840388</td>\n",
       "      <td>0.699417</td>\n",
       "      <td>0.792621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.846214</td>\n",
       "      <td>0.716893</td>\n",
       "      <td>0.764660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.852039</td>\n",
       "      <td>0.776311</td>\n",
       "      <td>0.801165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.853592</td>\n",
       "      <td>0.739806</td>\n",
       "      <td>0.801942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Bernoulli  Gaussian  _vanilla_\n",
       "0   0.850485  0.725049   0.793398\n",
       "1   0.840388  0.699417   0.792621\n",
       "2   0.846214  0.716893   0.764660\n",
       "3   0.852039  0.776311   0.801165\n",
       "4   0.853592  0.739806   0.801942"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = [('preprocessor', NaiveBayesPreprocessor(bins=20)),\n",
    "        ('nb', NaiveBayesClassifier())]\n",
    "pipe = Pipeline(pipe)\n",
    "nbs = {'_vanilla_': pipe,\n",
    "       # 'Multinomial': MultinomialNB(),\n",
    "       'Bernoulli': BernoulliNB(),\n",
    "       'Gaussian': GaussianNB()}\n",
    "\n",
    "X, y = activity_data[activity_data.columns[:-1]], activity_data[activity_data.columns[-1]]\n",
    "\n",
    "rs = ShuffleSplit(test_size=0.25, n_splits=5)\n",
    "scores = defaultdict(list)\n",
    "for train_index, test_index in rs.split(X):\n",
    "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "    for key, est in nbs.items():\n",
    "        est.fit(X_train, y_train)\n",
    "        scores[key].append(est.score(X_test, y_test))\n",
    "\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, `BernoulliNB` does well. This is because it binarizes the dataset prior to fitting the Bernoulli Naive Bayes, and the threshold it uses to binarize is 0. Incidentally, this works well with predicting the activity. In the prior example, this had almost no added value since everything was nonnegative.\n",
    "\n",
    "At this point, one could do some parameter tuning, play with the possible bin value etc. In any case, this dataset is not a great dataset for the Naive Bayes type algorithms, but I wanted to see how this implementation does in such an example.\n",
    "\n",
    "---\n",
    "\n",
    "Next, I will explore a weighted version of this implementation of Naive Bayes, and use it as a weak learner in a boosting scheme."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
